{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Dropping Data After Excel Cleaning\n",
    "- After performing extensive data cleaning in Excel and imputing as many missing values as possible, the next step for cleaning is to drop any remaining rows that still contain missing values â€” specifically, cells with NaN or \"No Value\" â€” from key columns. This ensures that the dataset is reliable and ready for accurate analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/Users/ayemaq/desktop/TEMPLATE_DA_Mod1Project/data/cleaned/03_Excel_Clean.xlsx\", sheet_name=\"Cleaned_Cafe_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>Items</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Price Per Unit</th>\n",
       "      <th>Grand total</th>\n",
       "      <th>Payment Method</th>\n",
       "      <th>Location</th>\n",
       "      <th>Transaction Date</th>\n",
       "      <th>Days_of_week</th>\n",
       "      <th>Location_was_unavailable</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Product</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TXN_1000555</td>\n",
       "      <td>Tea</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>1900-01-05</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cake</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TXN_1001832</td>\n",
       "      <td>Salad</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Takeaway</td>\n",
       "      <td>2023-10-19</td>\n",
       "      <td>1900-01-05</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TXN_1002457</td>\n",
       "      <td>Cookie</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Digital Wallet</td>\n",
       "      <td>Takeaway</td>\n",
       "      <td>2023-09-29</td>\n",
       "      <td>1900-01-06</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cookie</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TXN_1003246</td>\n",
       "      <td>Juice</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-02-15</td>\n",
       "      <td>1900-01-04</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Juice</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TXN_1004184</td>\n",
       "      <td>Smoothie</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-05-18</td>\n",
       "      <td>1900-01-05</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Salad</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Transaction ID     Items Quantity Price Per Unit Grand total  \\\n",
       "0    TXN_1000555       Tea        1            1.5         1.5   \n",
       "1    TXN_1001832     Salad        2              5          10   \n",
       "2    TXN_1002457    Cookie        5              1           5   \n",
       "3    TXN_1003246     Juice        2              3           6   \n",
       "4    TXN_1004184  Smoothie        1              4           4   \n",
       "\n",
       "   Payment Method  Location Transaction Date Days_of_week  \\\n",
       "0     Credit Card  In-store       2023-10-19   1900-01-05   \n",
       "1            Cash  Takeaway       2023-10-19   1900-01-05   \n",
       "2  Digital Wallet  Takeaway       2023-09-29   1900-01-06   \n",
       "3             NaN       NaN       2023-02-15   1900-01-04   \n",
       "4     Credit Card  In-store       2023-05-18   1900-01-05   \n",
       "\n",
       "   Location_was_unavailable  Unnamed: 10  Unnamed: 11 Product  Price  \n",
       "0                     False          NaN          NaN    Cake    3.0  \n",
       "1                     False          NaN          NaN  Coffee    2.0  \n",
       "2                     False          NaN          NaN  Cookie    1.0  \n",
       "3                      True          NaN          NaN   Juice    3.0  \n",
       "4                     False          NaN          NaN   Salad    5.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transaction ID                  0\n",
       "Items                         512\n",
       "Quantity                       18\n",
       "Price Per Unit                 38\n",
       "Grand total                    20\n",
       "Payment Method               3178\n",
       "Location                     3961\n",
       "Transaction Date                0\n",
       "Days_of_week                    0\n",
       "Location_was_unavailable        0\n",
       "Unnamed: 10                 10000\n",
       "Unnamed: 11                 10000\n",
       "Product                      9992\n",
       "Price                        9992\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.isna().sum() + (df == \"No Value\").sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ—‘ Dropping Rows with Missing or Invalid Entries\n",
    "Dropped all rows containing NaN or \"No Value\" from the following critical columns to ensure data quality and consistency:\n",
    "1. Items\n",
    "2. Quantity\n",
    "3. Price per Unit\n",
    "4. Grand Total\n",
    "\n",
    "This step helps eliminate incomplete or unreliable records before moving on to the analysis phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_clean = ['Items', 'Quantity', 'Price Per Unit', 'Grand total']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Attempt 1: Dropping Rows with \"No Value\" (Flexible Matching, Without Changing Data Types)\n",
    "This code filters out rows that contain \"No Value\" in any of the specified important columns (like Items, Quantity, etc.) â€” regardless of how it's typed (e.g., \"NO VALUE\", \" no value \", etc.). It preserves the original data types of the DataFrame.\n",
    "\n",
    "### ðŸ” Breakdown:\n",
    "- df[cols_to_clean]\n",
    "    - Select only the columns we're focused on cleaning (e.g., Items, Quantity, etc.).\n",
    "- .apply(..., axis=1)\n",
    "    - Apply the function row by row.\n",
    "- lambda row: ...\n",
    "    - For each row, convert each cell to a string, strip whitespace, make lowercase, and check if it equals \"no value\".\n",
    "- .any(axis=1)\n",
    "    - For each row, check: does any cell in this row equal \"no value\"?\n",
    "- ~ (tilde)\n",
    "    - Reverses the logic â€” instead of keeping rows with \"no value\", we exclude them.\n",
    "- df[...]\n",
    "    - Filters the DataFrame using the condition and assigns the cleaned data back to df.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop rows with \"No Value\" (case-insensitive string match)\n",
    "#df = df[~df[cols_to_clean].apply( # ~ Keep only the rows that do NOT have \"no value\" in them.  .apply: This applies a function to each row (axis=1 = row-wise).\n",
    "#    lambda row: row.astype(str).str.strip().str.lower().eq(\"no value\"), axis=1 # This defines what to do for each row of those selected columns.\n",
    "#).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Drop NaNs in those columns\n",
    "#df.dropna(subset=cols_to_clean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Drop rows with 0 in Quantity, Price Per Unit, or Grand total\n",
    "#df = df[~((df['Quantity'] == 0) | (df['Price Per Unit'] == 0) | (df['Grand total'] == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Reset index\n",
    "# df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2: Dropping Rows Based on the Number of \"Bad\" Values (\n",
    "This approach counts how many \"bad values\" (like NaN, \"no value\", or blank entries) exist per row in the four most important columns:\n",
    "- Items, Quantity, Price Per Unit, and Grand total.\n",
    "- Then we remove rows that have 2 or more bad values (i.e., we keep rows that have at least 3 good values).e can also confirm that the code is working as expected by printing how many rows were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cols_to_check = [\"Items\", \"Quantity\", \"Price Per Unit\", \"Grand total\"]\\n\\n# Check how many \\'bad\\' values are in each row for those 4 columns\\nbad_counts = df[cols_to_check].apply(\\n    lambda row: sum((pd.isna(row)) | (row.astype(str).str.strip().str.lower() == \"no value\")),\\n    axis=1\\n)\\n\\n# Count how many rows had 3 or more bad values\\nprint(\"Rows with 2 or more bad values in important columns:\", (bad_counts >= 2).sum())\\n\\n# df = df[bad_counts < 2]  # keep rows with 0, 1, or bad values\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''cols_to_check = [\"Items\", \"Quantity\", \"Price Per Unit\", \"Grand total\"]\n",
    "\n",
    "# Check how many 'bad' values are in each row for those 4 columns\n",
    "bad_counts = df[cols_to_check].apply(\n",
    "    lambda row: sum((pd.isna(row)) | (row.astype(str).str.strip().str.lower() == \"no value\")),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Count how many rows had 3 or more bad values\n",
    "print(\"Rows with 2 or more bad values in important columns:\", (bad_counts >= 2).sum())\n",
    "\n",
    "# df = df[bad_counts < 2]  # keep rows with 0, 1, or bad values\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 3: Drop Rows Only When They Are Likely Unusable\n",
    "In this logic, we drop a row only if both of the following conditions are met:\n",
    "- Either Payment Method or Location is blank, AND There are 2 or more â€œbadâ€ values in the important columns:\n",
    "    - Items, Quantity, Price Per Unit (PPU), and Grand total.\n",
    "- â€œBadâ€ values include:\n",
    "    - \"No value\"\n",
    "    - 0\n",
    "    - Empty cells\n",
    "    - NaN\n",
    "\n",
    "\n",
    "EXAMPLE\n",
    "\n",
    "\n",
    "| Item | Quantity | PPU | GT | Location | PM       | Action                                       |\n",
    "| ---- | -------- | --- | -- | -------- | -------- | -------------------------------------------- |\n",
    "| 0    | NA       | 2   | 2  | No value | No value | âœ… Drop (2+ bad values + missing location/PM) |\n",
    "| 1    | 0        | 2   | 2  | No value | No value | âŒ Keep (only 1 bad value in important cols)  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = [\"Items\", \"Quantity\", \"Price Per Unit\", \"Grand total\"]\n",
    "\n",
    "def is_blank(val):\n",
    "    return pd.isna(val) or str(val).strip() == \"\"\n",
    "\n",
    "df = df[~df.apply(\n",
    "    lambda row: (\n",
    "        # Check if either Payment Method or Location is blank\n",
    "        is_blank(row[\"Payment Method\"]) or is_blank(row[\"Location\"])\n",
    "    ) and (\n",
    "        # Check if 2 or more of the important columns are bad\n",
    "        sum(is_blank(row[col]) or str(row[col]).strip().lower() == \"no value\" or row[col] == 0 for col in cols_to_check) >= 2\n",
    "    ),\n",
    "    axis=1\n",
    ")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows that still match the bad condition: 0\n"
     ]
    }
   ],
   "source": [
    "# to check\n",
    "# Redefine is_blank for use here too\n",
    "def is_blank(val):\n",
    "    return pd.isna(val) or str(val).strip() == \"\"\n",
    "\n",
    "# Check for rows that should have been dropped â€” sanity check\n",
    "bad_rows = df[df.apply(\n",
    "    lambda row: (\n",
    "        is_blank(row[\"Payment Method\"]) or is_blank(row[\"Location\"])\n",
    "    ) and (\n",
    "        sum(is_blank(row[col]) or str(row[col]).strip().lower() == \"no value\" or row[col] == 0 for col in cols_to_check) >= 2\n",
    "    ),\n",
    "    axis=1\n",
    ")]\n",
    "\n",
    "# Print how many got through (should be 0)\n",
    "print(\"Rows that still match the bad condition:\", len(bad_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"/Users/ayemaq/desktop/TEMPLATE_DA_Mod1Project/data/cleaned/cafe_data.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
